{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>6.4 Stacking<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#6.4.3-Stacking算法实现\" data-toc-modified-id=\"6.4.3-Stacking算法实现-1\">6.4.3 Stacking算法实现</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4.3 Stacking算法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) Age    0.308631\n",
      " 2) Embarked 0.174540\n",
      " 3) Fare   0.174388\n",
      " 4) Parch  0.086825\n",
      " 5) Pclass 0.085883\n",
      " 6) Sex    0.081620\n",
      "\n",
      "\n",
      "['Age', 'Embarked', 'Fare', 'Parch', 'Pclass', 'Sex']\n",
      "\n",
      "\n",
      "0.9719529085872577\n"
     ]
    }
   ],
   "source": [
    "#coding:utf8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve,auc   \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "def get_stacking(clf, x_train, y_train, x_test, n_folds=5):\n",
    "    train_num, test_num = x_train.shape[0], x_test.shape[0]\n",
    "    second_level_train_set = np.zeros((train_num,))\n",
    "    second_level_test_set = np.zeros((test_num,))\n",
    "    test_nfolds_sets = np.zeros((test_num, n_folds))\n",
    "    kf = KFold(n_splits=n_folds)\n",
    "\n",
    "    for i,(train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "        x_tra, y_tra = x_train[train_index], y_train[train_index]\n",
    "        x_tst, y_tst =  x_train[test_index], y_train[test_index]\n",
    "\n",
    "        clf.fit(x_tra, y_tra)\n",
    "\n",
    "        second_level_train_set[test_index] = clf.predict_proba(x_tst)[:,1]\n",
    "        test_nfolds_sets[:,i] = clf.predict_proba(x_test)[:,1]\n",
    "\n",
    "\n",
    "    second_level_test_set[:] = test_nfolds_sets.mean(axis=1)\n",
    "    return second_level_train_set, second_level_test_set\n",
    "\n",
    "\n",
    "# 导入数据\n",
    "train_set = pd.read_csv('../data/train.csv')\n",
    "test_set = pd.read_csv('../data/test.csv')\n",
    "\n",
    "# 特征含义\n",
    "# PassengerId 乘客编号\n",
    "# Survived 是否幸存\n",
    "# Pclass 船票等级\n",
    "# Name 乘客姓名\n",
    "# Sex 乘客性别\n",
    "# Age 乘客年龄\n",
    "# SibSp 兄弟姐妹/配偶数量\n",
    "# Parch 父母/子女数量\n",
    "# Ticket 船票号码\n",
    "# Fare 船票价格\n",
    "# Cabin 船舱\n",
    "# Embarked 登录港口\n",
    "\n",
    "        \n",
    "# print(train_set.info())\n",
    "# print('*'*40)\n",
    "# print(test_set.info())\n",
    "# print('\\n')\n",
    "# 训练集测试集合并\n",
    "train_test = pd.concat([train_set,test_set],axis=0)\n",
    "# print(train_test.info())\n",
    "\n",
    "\n",
    "# print(train_test['Survived'].value_counts())\n",
    "\n",
    "# print(train_test.describe().T)\n",
    "\n",
    "# train_test_corr = train_test.corr()\n",
    "# plt.subplots(figsize=(12,7))\n",
    "# sns.heatmap(train_test_corr,vmin=-1,annot=True,square=True)\n",
    "# plt.show()\n",
    "# print('\\n')\n",
    "\n",
    "train_test['Embarked'].value_counts()\n",
    "train_test['Pclass'].value_counts()\n",
    "train_test['Embarked'].fillna('S',inplace=True)\n",
    "\n",
    "#票价与pclass和Embarked有关\n",
    "train_test.groupby(['Pclass','Embarked']).Fare.mean()\n",
    "train_test['Fare'].fillna(14.435422,inplace=True)\n",
    "\n",
    "# 缺失值填充\n",
    "train_test['Age'].fillna(train_test['Age'].median(),inplace=True)\n",
    "\n",
    "\n",
    "# 特征工程\n",
    "train_test['SibSp_Parch'] = train_test['Parch'] + train_test['SibSp']\n",
    "\n",
    "#从名字中提取出称呼\n",
    "train_test['Name_new'] = train_test['Name'].str.extract('.+,(.+)',expand=False).str.extract('^(.+?)\\.',expand=False).str.strip()\n",
    "# print(train_test['Name_new'].unique())\n",
    "# print('\\n')\n",
    "\n",
    "#将姓名分类处理()\n",
    "train_test['Name_new'].replace(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 'Officer' , inplace = True)\n",
    "train_test['Name_new'].replace(['Jonkheer', 'Don', 'Sir', 'the Countess', 'Dona', 'Lady'], 'Royalty' , inplace = True)\n",
    "train_test['Name_new'].replace(['Mme', 'Ms', 'Mrs'], 'Mrs',inplace=True)\n",
    "train_test['Name_new'].replace(['Mlle', 'Miss'], 'Miss',inplace=True)\n",
    "train_test['Name_new'].replace(['Mr'], 'Mr' , inplace = True)\n",
    "train_test['Name_new'].replace(['Master'], 'Master' , inplace = True)\n",
    "# print(train_test['Name_new'].unique())\n",
    "# print('\\n')\n",
    "\n",
    "# 分类变量数值化\n",
    "train_test['Name_new'] = train_test['Name_new'].map({'Mr':0,'Mrs':1,'Miss':2,'Master':3,'Royalty':4,'Officer':5}).astype(int)\n",
    "train_test['Sex'] = train_test['Sex'].map({'female':1,'male':0}).astype(int)\n",
    "train_test['Embarked'] = train_test['Embarked'].map({'S':0,'C':1,'Q':2}).astype(int)\n",
    "\n",
    "\n",
    "#将年龄划分阶段\n",
    "train_test['Age']=pd.cut(train_test['Age'],bins=[0,18,30,40,50,100],labels=[1,2,3,4,5])\n",
    "train_test['Age'] = train_test['Age'].astype('float64')\n",
    "\n",
    "# 剔除不需要的特征\n",
    "train_test.drop(['PassengerId','Ticket','Name','Cabin'],axis=1,inplace=True)\n",
    "# print(train_test.info())\n",
    "\n",
    "#特征工程完成，划分数据集\n",
    "train_data=train_test[:891]\n",
    "test_data=train_test[891:]\n",
    "train_data_X=train_data.drop(['Survived'],axis=1)\n",
    "train_data_Y=train_data['Survived']\n",
    "test_data_X=test_data.drop(['Survived'],axis=1)\n",
    "test_data_Y=test_data['Survived']\n",
    "\n",
    "\n",
    "# 特征选择\n",
    "# 随机森林评估特征重要性\n",
    "feat_labels=train_data_X.columns\n",
    "forest=RandomForestClassifier(n_estimators=10,n_jobs=-1,random_state=2019) \n",
    "forest.fit(train_data_X,train_data_Y)\n",
    "importances=forest.feature_importances_\n",
    "indices=np.argsort(importances)[::-1]  #排序取反\n",
    "var_list = []\n",
    "for f in range(6):\n",
    "    print (\"%2d) %-*s %f\" % (f+1,6,feat_labels[f],importances[indices[f]]) )\n",
    "    var_list.append(feat_labels[f])\n",
    "print('\\n')\n",
    "print(var_list)\n",
    "print('\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "# 基分类器\n",
    "lr_model = LogisticRegression(random_state=2019)       #逻辑回归\n",
    "dt_model = DecisionTreeClassifier(random_state=2019)   #决策树\n",
    "rf_model = RandomForestClassifier(random_state=2019)    #随机森林\n",
    "\n",
    "lr_model.fit(train_data_X[var_list],train_data_Y)\n",
    "dt_model.fit(train_data_X[var_list],train_data_Y)\n",
    "rf_model.fit(train_data_X[var_list],train_data_Y)\n",
    "\n",
    "\n",
    "#模型融合\n",
    "x_train_arr = np.array(train_data_X[var_list])\n",
    "y_train_arr = np.array(train_data_Y)\n",
    "x_test_arr = np.array(test_data_X[var_list])\n",
    "\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "for clf in [lr_model,dt_model,rf_model]:\n",
    "    train_set, test_set = get_stacking(clf, x_train_arr, y_train_arr, x_test_arr)\n",
    "    train_sets.append(train_set)\n",
    "    test_sets.append(test_set)\n",
    "\n",
    "\n",
    "meta_train = np.concatenate([result_set.reshape(-1,1) for result_set in train_sets], axis = 1)  #np.concatenate,axis=1列连接\n",
    "meta_test = np.concatenate([y_test_set.reshape(-1,1) for y_test_set in test_sets], axis = 1)\n",
    "\n",
    "# #各模型预测结果相关性\n",
    "# m_train=pd.DataFrame(meta_train,columns=['lr','dt','rf'])\n",
    "# m_train.astype(float).corr() \n",
    "\n",
    "\n",
    "#使用lr作为我们的次级分类器\n",
    "meta_model = LogisticRegression(random_state = 2019)\n",
    "meta_model.fit(meta_train, y_train_arr)\n",
    "prediction = meta_model.predict_proba(meta_test)\n",
    "#print(prediction)\n",
    "\n",
    "\n",
    "false_positive_rate, recall, thresholds = roc_curve(test_data_Y, prediction[:, 1])\n",
    "final_auc = auc(false_positive_rate,recall)\n",
    "print(final_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "6.4 Stacking",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
